import { type ActionFunctionArgs } from '@remix-run/cloudflare';
import { createDataStream, generateId } from 'ai';
import { MAX_RESPONSE_SEGMENTS, MAX_TOKENS, type FileMap } from '~/lib/.server/llm/constants';
import { CONTINUE_PROMPT } from '~/lib/common/prompts/prompts';
import { streamText, type Messages, type StreamingOptions } from '~/lib/.server/llm/stream-text';
import SwitchableStream from '~/lib/.server/llm/switchable-stream';
import type { IProviderSetting } from '~/types/model';
import { createScopedLogger } from '~/utils/logger';
import { getFilePaths, selectContext } from '~/lib/.server/llm/select-context';
import type {
  AgentCommentaryAnnotation,
  AgentCommentaryPhase,
  AgentRunMetricsDataEvent,
  CheckpointDataEvent,
  ContextAnnotation,
  ProjectMemoryDataEvent,
  ProgressAnnotation,
  SubAgentEvent,
  UsageDataEvent,
} from '~/types/context';
import { WORK_DIR } from '~/utils/constants';
import { createSummary } from '~/lib/.server/llm/create-summary';
import { extractPropertiesFromMessage } from '~/lib/.server/llm/utils';
import type { DesignScheme } from '~/types/design-scheme';
import { MCPService } from '~/lib/services/mcpService';
import { AgentRecoveryController } from '~/lib/.server/llm/agent-recovery';
import { StreamRecoveryManager } from '~/lib/.server/llm/stream-recovery';
import { recordAgentRunMetrics } from '~/lib/.server/llm/run-metrics';
import { deriveProjectMemoryKey, getProjectMemory, upsertProjectMemory } from '~/lib/.server/llm/project-memory';
import { analyzeRunContinuation } from '~/lib/.server/llm/run-continuation';
import { SubAgentManager, type SubAgentConfig, type SubAgentState } from '~/lib/.server/llm/sub-agent';
import { createPlannerExecutor } from '~/lib/.server/llm/sub-agent/planner-executor';
import { resolveRuntimeEnv } from '~/lib/.server/runtime-env';
import { addUsageTotals } from '~/lib/runtime/usage';
import { enforceCommentaryContract } from '~/lib/runtime/commentary-contract';
import { extractCheckpointEvents, extractExecutionFailure } from '~/lib/runtime/checkpoint-events';
import { COMMENTARY_HEARTBEAT_INTERVAL_MS, buildCommentaryHeartbeat } from '~/lib/runtime/commentary-heartbeat';
import { getCommentaryPoolMessage } from '~/lib/runtime/commentary-pool.generated';
import { LLMManager } from '~/lib/modules/llm/manager';
import { hydrateApiKeysFromRuntimeEnv, mergeAndSanitizeApiKeys } from '~/lib/.server/llm/api-key-utils';
import { normalizeCredential } from '~/lib/runtime/credentials';
import {
  ensureLatestUserMessageSelectionEnvelope,
  resolvePreferredModelProvider,
  sanitizeSelectionWithApiKeys,
} from '~/lib/.server/llm/message-selection';

export async function action(args: ActionFunctionArgs) {
  return chatAction(args);
}

const logger = createScopedLogger('api.chat');
const MAX_RUN_CONTINUATION_ATTEMPTS = 5;
const LONG_THINK_MODEL_RE =
  /\b(gpt-5|gpt-5\.2|gpt-5-codex|codex|o1|o3|claude-3\.7|claude-3\.5-sonnet-latest|claude-3-5-sonnet-latest)\b/i;

function parseBooleanEnv(value: string | undefined, fallback: boolean): boolean {
  if (value == null) {
    return fallback;
  }

  const normalized = value.trim().toLowerCase();

  if (['1', 'true', 'yes', 'on'].includes(normalized)) {
    return true;
  }

  if (['0', 'false', 'no', 'off'].includes(normalized)) {
    return false;
  }

  return fallback;
}

function buildForcedRunRecoveryPrompt(params: { provider: string; model: string; originalRequest: string }): string {
  const { provider, model, originalRequest } = params;

  return `[Model: ${model}]

[Provider: ${provider}]

Forced recovery mode: previous attempts did not produce a runnable implementation.
Continue from the current project state and execute the original request now.

Original request:
${originalRequest}

Hard requirements (must follow exactly):
1) Emit exactly one <boltArtifact> with executable <boltAction> steps.
2) Do NOT run inspection-only commands (ls, pwd, cat, find, tree, env, echo).
3) Do NOT re-scaffold if package.json already exists.
4) First meaningful actions must be <boltAction type="file"> updates that implement the requested features.
5) If dependencies are missing, run a single install command.
6) Include a <boltAction type="start"> command to run the dev server.
7) If preview still shows fallback starter text, replace src/App.tsx (or equivalent root UI entry) immediately.
8) Keep prose short; focus on executable steps and completion.`;
}

function extractLatestUserGoal(messages: Messages): string {
  const lastUser = [...messages].reverse().find((message) => message.role === 'user');

  if (!lastUser) {
    return '';
  }

  const { content } = extractPropertiesFromMessage(lastUser);

  return content || lastUser.content || '';
}

function detectManualIntervention(messages: Messages): boolean {
  const lastUser = [...messages].reverse().find((message) => message.role === 'user');

  if (!lastUser) {
    return false;
  }

  const text = (lastUser.content || '').toLowerCase();
  const hasContinueCue =
    text.includes('\ncontinue') ||
    text.includes('please continue') ||
    text.includes('go on') ||
    text.includes('resume from');

  const partIntervention =
    Array.isArray(lastUser.parts) &&
    lastUser.parts.some((part) => {
      if (part.type !== 'tool-invocation') {
        return false;
      }

      return part.toolInvocation?.state === 'result';
    });

  return hasContinueCue || partIntervention;
}

function parseCookies(cookieHeader: string): Record<string, string> {
  const cookies: Record<string, string> = {};

  const items = cookieHeader.split(';').map((cookie) => cookie.trim());

  items.forEach((item) => {
    const [name, ...rest] = item.split('=');

    if (name && rest) {
      const decodedName = decodeURIComponent(name.trim());
      const decodedValue = decodeURIComponent(rest.join('=').trim());
      cookies[decodedName] = decodedValue;
    }
  });

  return cookies;
}

function parseJsonObject<T extends Record<string, any>>(raw: string | undefined, fallback: T): T {
  if (!raw) {
    return fallback;
  }

  try {
    const parsed = JSON.parse(raw);

    if (!parsed || typeof parsed !== 'object' || Array.isArray(parsed)) {
      return fallback;
    }

    return parsed as T;
  } catch {
    return fallback;
  }
}

async function chatAction({ context, request }: ActionFunctionArgs) {
  const requestPayload = await request.json<{
    messages: Messages;
    files: any;
    promptId?: string;
    contextOptimization: boolean;
    chatMode: 'discuss' | 'build';
    designScheme?: DesignScheme;
    supabase?: {
      isConnected: boolean;
      hasSelectedProject: boolean;
      credentials?: {
        anonKey?: string;
        supabaseUrl?: string;
      };
    };
    maxLLMSteps: number;
    projectMemory?: {
      projectKey: string;
      summary: string;
      architecture: string;
      latestGoal: string;
      runCount: number;
      updatedAt: string;
    } | null;
    apiKeys?: Record<string, string>;
    providerSettings?: Record<string, IProviderSetting>;
    selectedModel?: string;
    selectedProvider?: string;
  }>();
  const {
    messages,
    files,
    promptId,
    contextOptimization,
    supabase,
    chatMode,
    designScheme,
    maxLLMSteps,
    projectMemory,
    apiKeys: bodyApiKeys = {},
    providerSettings: bodyProviderSettings = {},
    selectedModel: selectedModelBody,
    selectedProvider: selectedProviderBody,
  } = requestPayload;

  const cookieHeader = request.headers.get('Cookie');
  const parsedCookies = parseCookies(cookieHeader || '');
  const cookieApiKeys = parseJsonObject<Record<string, string>>(parsedCookies.apiKeys, {});
  const cookieProviderSettings = parseJsonObject<Record<string, IProviderSetting>>(parsedCookies.providers, {});
  const selectedModelCookie = parsedCookies.selectedModel;
  const selectedProviderCookie = parsedCookies.selectedProvider;
  const selectedModel = selectedModelBody || selectedModelCookie;
  const selectedProvider = selectedProviderBody || selectedProviderCookie;
  const runtimeEnv = resolveRuntimeEnv(context.cloudflare?.env as unknown as Record<string, unknown> | undefined);
  const llmManager = LLMManager.getInstance(runtimeEnv as any);
  const providerTokenKeyByName = Object.fromEntries(
    llmManager.getAllProviders().map((provider) => [provider.name, provider.config.apiTokenKey]),
  );
  const mergedApiKeys = mergeAndSanitizeApiKeys({
    cookieApiKeys,
    bodyApiKeys,
  });
  const apiKeys = hydrateApiKeysFromRuntimeEnv({
    apiKeys: mergedApiKeys,
    runtimeEnv,
    providerTokenKeyByName,
  });
  const providerSettings: Record<string, IProviderSetting> = {
    ...cookieProviderSettings,
    ...bodyProviderSettings,
  };

  const stream = new SwitchableStream();

  const cumulativeUsage = {
    completionTokens: 0,
    promptTokens: 0,
    totalTokens: 0,
  };
  const requestStartedAt = Date.now();
  const runId = generateId();
  const requestUrl = new URL(request.url);
  const requestDebugContext = {
    runId,
    route: requestUrl.pathname,
    messageCount: messages.length,
    latestRole: messages[messages.length - 1]?.role,
    selectedModel,
    selectedProvider,
    hasCookieApiKeys: Object.keys(cookieApiKeys).length > 0,
    hasBodyApiKeys: Object.keys(bodyApiKeys).length > 0,
    hasMergedApiKeys: Object.keys(mergedApiKeys).length > 0,
    hasResolvedApiKeys: Object.keys(apiKeys).length > 0,
    mergedApiKeyProviders: Object.keys(mergedApiKeys),
    resolvedApiKeyProviders: Object.keys(apiKeys),
    selectedProviderHasMergedKey: Boolean(
      selectedProvider ? normalizeCredential(mergedApiKeys[selectedProvider]) : undefined,
    ),
    selectedProviderHasResolvedKey: Boolean(
      selectedProvider ? normalizeCredential(apiKeys[selectedProvider]) : undefined,
    ),
    hasOpenAIEnvKey: Boolean(normalizeCredential(runtimeEnv.OPENAI_API_KEY)),
    chatMode,
    maxLLMSteps,
  };
  let resolvedSelectionForLogs: {
    provider?: string;
    model?: string;
  } = {
    provider: selectedProvider,
    model: selectedModel,
  };
  const manualInterventionDetected = detectManualIntervention(messages);
  const latestUserGoal = extractLatestUserGoal(messages);
  const projectKey = deriveProjectMemoryKey(files);
  const cachedProjectMemory = getProjectMemory(projectKey);
  const effectiveProjectMemory =
    projectMemory && projectMemory.projectKey === projectKey ? projectMemory : cachedProjectMemory;
  const envVars = runtimeEnv as Record<string, string | undefined>;
  const subAgentManager = SubAgentManager.getInstance();
  const encoder: TextEncoder = new TextEncoder();
  let progressCounter: number = 1;
  let stopCommentaryHeartbeatHandle: (() => void) | null = null;
  const stopHeartbeatIfRunning = () => {
    if (typeof stopCommentaryHeartbeatHandle === 'function') {
      stopCommentaryHeartbeatHandle();
    }

    stopCommentaryHeartbeatHandle = null;
  };

  try {
    logger.info(`chat request started ${JSON.stringify(requestDebugContext)}`);

    const mcpService = MCPService.getInstance();
    const totalMessageContent = messages.reduce((acc, message) => acc + message.content, '');
    logger.debug(`Total message length: ${totalMessageContent.split(' ').length}, words`);

    let lastChunk: string | undefined = undefined;

    const dataStream = createDataStream({
      async execute(dataStream) {
        let firstCommentaryAt: number | null = null;
        let lastCommentaryPhase: AgentCommentaryPhase = 'plan';
        let commentaryHeartbeat: ReturnType<typeof setInterval> | null = null;
        let streamRecovery: StreamRecoveryManager | null = null;
        let recoveryTriggered = false;
        let recoverySucceeded = false;
        let completionEmitted = false;
        let hasExecutionFailures = false;
        let latestExecutionFailure: ReturnType<typeof extractExecutionFailure> = null;

        const stopCommentaryHeartbeat = () => {
          if (commentaryHeartbeat) {
            clearInterval(commentaryHeartbeat);
            commentaryHeartbeat = null;
          }
        };
        stopCommentaryHeartbeatHandle = stopCommentaryHeartbeat;

        const markRunActivity = () => {
          streamRecovery?.updateActivity();
        };

        const writeCommentary = (
          phase: AgentCommentaryPhase,
          message: string,
          status: AgentCommentaryAnnotation['status'] = 'in-progress',
          detail?: string,
        ) => {
          if (firstCommentaryAt === null) {
            firstCommentaryAt = Date.now();
          }

          lastCommentaryPhase = phase;

          const order = progressCounter++;
          const fallbackMessage = message || 'I am still working and will post another update shortly.';
          const effectiveMessage =
            status === 'in-progress'
              ? getCommentaryPoolMessage(phase, requestStartedAt + order, fallbackMessage)
              : fallbackMessage;

          const contracted = enforceCommentaryContract({
            phase,
            message: effectiveMessage,
            detail,
          });

          const payload: AgentCommentaryAnnotation = {
            type: 'agent-commentary',
            phase,
            status,
            order,
            message: contracted.message,
            timestamp: new Date().toISOString(),
            detail: contracted.detail,
          };

          dataStream.writeData({
            ...payload,
          });
        };

        const startCommentaryHeartbeat = () => {
          if (commentaryHeartbeat) {
            return;
          }

          commentaryHeartbeat = setInterval(() => {
            const heartbeat = buildCommentaryHeartbeat(Date.now() - requestStartedAt, lastCommentaryPhase);
            writeCommentary(heartbeat.phase, heartbeat.message, 'in-progress', heartbeat.detail);
          }, COMMENTARY_HEARTBEAT_INTERVAL_MS);
        };

        const recoveryController = new AgentRecoveryController();
        let pendingRecoveryReason: string | undefined = undefined;
        let pendingRecoveryBackoffMs = 0;
        let forceFinalizeRequested = false;

        const emitRunCompletionEvents = (finalAssistantText: string, model: string, provider: string) => {
          if (completionEmitted) {
            return;
          }

          stopCommentaryHeartbeat();
          completionEmitted = true;

          const commentaryFirstEventLatencyMs =
            firstCommentaryAt === null ? null : firstCommentaryAt - requestStartedAt;
          const projectMemoryEntry = upsertProjectMemory({
            projectKey,
            files,
            latestGoal: latestUserGoal,
            summary: summary || finalAssistantText,
          });
          const aggregate = recordAgentRunMetrics({
            runId,
            provider,
            model,
            commentaryFirstEventLatencyMs,
            recoveryTriggered,
            recoverySucceeded,
            manualIntervention: manualInterventionDetected,
            timestamp: new Date().toISOString(),
          });

          const usageDataEvent: UsageDataEvent = {
            type: 'usage',
            completionTokens: cumulativeUsage.completionTokens,
            promptTokens: cumulativeUsage.promptTokens,
            totalTokens: cumulativeUsage.totalTokens,
            timestamp: new Date().toISOString(),
          };
          const runMetricsEvent: AgentRunMetricsDataEvent = {
            type: 'run-metrics',
            runId,
            provider,
            model,
            commentaryFirstEventLatencyMs,
            recoveryTriggered,
            recoverySucceeded,
            manualIntervention: manualInterventionDetected,
            timestamp: new Date().toISOString(),
            aggregate,
          };
          const projectMemoryEvent: ProjectMemoryDataEvent = {
            type: 'project-memory',
            projectKey: projectMemoryEntry.projectKey,
            summary: projectMemoryEntry.summary,
            architecture: projectMemoryEntry.architecture,
            latestGoal: projectMemoryEntry.latestGoal,
            runCount: projectMemoryEntry.runCount,
            updatedAt: projectMemoryEntry.updatedAt,
          };

          dataStream.writeData({ ...usageDataEvent });
          dataStream.writeMessageAnnotation({
            type: 'usage',
            value: {
              completionTokens: cumulativeUsage.completionTokens,
              promptTokens: cumulativeUsage.promptTokens,
              totalTokens: cumulativeUsage.totalTokens,
            },
          });
          dataStream.writeData({ ...runMetricsEvent });
          dataStream.writeData({ ...projectMemoryEvent });
          dataStream.writeData({
            type: 'progress',
            label: 'response',
            status: 'complete',
            order: progressCounter++,
            message: hasExecutionFailures ? 'Response Generated (with execution failures)' : 'Response Generated',
          } satisfies ProgressAnnotation);
        };

        const longThinkSelectedModel = selectedModel || '';
        const dynamicStreamTimeoutFallbackMs = LONG_THINK_MODEL_RE.test(longThinkSelectedModel) ? 300000 : 180000;
        const configuredStreamTimeoutMs = Number(
          envVars?.BOLT_STREAM_TIMEOUT_MS || process?.env?.BOLT_STREAM_TIMEOUT_MS || dynamicStreamTimeoutFallbackMs,
        );
        const configuredStreamMaxRetries = Number(
          envVars?.BOLT_STREAM_RECOVERY_MAX_RETRIES || process?.env?.BOLT_STREAM_RECOVERY_MAX_RETRIES || 2,
        );
        const streamTimeoutMs =
          Number.isFinite(configuredStreamTimeoutMs) && configuredStreamTimeoutMs >= 30000
            ? configuredStreamTimeoutMs
            : dynamicStreamTimeoutFallbackMs;
        const streamMaxRetries =
          Number.isFinite(configuredStreamMaxRetries) && configuredStreamMaxRetries >= 0
            ? configuredStreamMaxRetries
            : 2;

        let activeStreamAbortController: AbortController | null = null;
        const createStreamAbortSignal = () => {
          activeStreamAbortController = new AbortController();
          return activeStreamAbortController.signal;
        };
        const stopRunMonitors = () => {
          streamRecovery?.stop();
          stopCommentaryHeartbeat();
          activeStreamAbortController = null;
        };
        streamRecovery = new StreamRecoveryManager({
          timeout: streamTimeoutMs,
          maxRetries: streamMaxRetries,
          onTimeout: () => {
            const signal = recoveryController.registerTimeout();
            pendingRecoveryReason = pendingRecoveryReason || signal.reason;
            pendingRecoveryBackoffMs = Math.max(pendingRecoveryBackoffMs, signal.backoffMs);
            forceFinalizeRequested = forceFinalizeRequested || signal.forceFinalize;
            recoveryTriggered = true;
            writeCommentary('recovery', signal.message, 'warning', signal.detail);
            logger.warn('Stream timeout - attempting recovery');

            if (activeStreamAbortController && !activeStreamAbortController.signal.aborted) {
              activeStreamAbortController.abort(
                new Error(`BOLT_STREAM_TIMEOUT: no stream activity for ${streamTimeoutMs}ms`),
              );
            }
          },
        });
        streamRecovery.startMonitoring();

        const filePaths = getFilePaths(files || {});
        let filteredFiles: FileMap | undefined = undefined;
        let summary: string | undefined = undefined;
        let messageSliceId = 0;
        const processedMessages = await mcpService.processToolInvocations(messages, dataStream);
        const preferredSelection = resolvePreferredModelProvider(processedMessages, selectedModel, selectedProvider);

        /*
         * Use explicit user-provided keys (cookie/body) for selection fallback decisions.
         * Runtime-env hydration still applies at execution time, but we avoid silently
         * switching to unrelated env-backed providers during model/provider sanitization.
         */
        const sanitizedSelection = sanitizeSelectionWithApiKeys({
          selection: preferredSelection,
          apiKeys: mergedApiKeys,
          selectedProviderCookie: selectedProvider,
        });
        resolvedSelectionForLogs = {
          provider: sanitizedSelection.provider,
          model: sanitizedSelection.model,
        };
        ensureLatestUserMessageSelectionEnvelope(processedMessages, sanitizedSelection);

        const collectedToolOutputs: string[] = [];
        let forceFinalizeAttempted = false;
        let runContinuationAttempts = 0;
        let forcedRunRecoveryAttempted = false;

        writeCommentary('plan', 'I am reviewing your request and mapping out the first steps.');
        startCommentaryHeartbeat();

        if (processedMessages.length > 3) {
          messageSliceId = processedMessages.length - 3;
        }

        if (filePaths.length > 0 && contextOptimization) {
          logger.debug('Generating Chat Summary');
          writeCommentary('plan', 'I am quickly summarizing recent context so I can stay on track.');
          dataStream.writeData({
            type: 'progress',
            label: 'summary',
            status: 'in-progress',
            order: progressCounter++,
            message: 'Analysing Request',
          } satisfies ProgressAnnotation);

          // Create a summary of the chat
          console.log(`Messages count: ${processedMessages.length}`);

          summary = await createSummary({
            messages: [...processedMessages],
            env: runtimeEnv as any,
            apiKeys,
            providerSettings,
            promptId,
            contextOptimization,
            onFinish(resp) {
              if (resp.usage) {
                logger.debug('createSummary token usage', JSON.stringify(resp.usage));
                addUsageTotals(cumulativeUsage, resp.usage as any);
              }
            },
          });
          dataStream.writeData({
            type: 'progress',
            label: 'summary',
            status: 'complete',
            order: progressCounter++,
            message: 'Analysis Complete',
          } satisfies ProgressAnnotation);

          dataStream.writeMessageAnnotation({
            type: 'chatSummary',
            summary,
            chatId: processedMessages.slice(-1)?.[0]?.id,
          } as ContextAnnotation);

          // Update context buffer
          logger.debug('Updating Context Buffer');
          writeCommentary('plan', 'I am selecting the files that matter for this task.');
          dataStream.writeData({
            type: 'progress',
            label: 'context',
            status: 'in-progress',
            order: progressCounter++,
            message: 'Determining Files to Read',
          } satisfies ProgressAnnotation);

          // Select context files
          console.log(`Messages count: ${processedMessages.length}`);
          filteredFiles = await selectContext({
            messages: [...processedMessages],
            env: runtimeEnv as any,
            apiKeys,
            files,
            providerSettings,
            promptId,
            contextOptimization,
            summary,
            onFinish(resp) {
              if (resp.usage) {
                logger.debug('selectContext token usage', JSON.stringify(resp.usage));
                addUsageTotals(cumulativeUsage, resp.usage as any);
              }
            },
          });

          if (filteredFiles) {
            logger.debug(`files in context : ${JSON.stringify(Object.keys(filteredFiles))}`);
          }

          dataStream.writeMessageAnnotation({
            type: 'codeContext',
            files: Object.keys(filteredFiles).map((key) => {
              let path = key;

              if (path.startsWith(WORK_DIR)) {
                path = path.replace(WORK_DIR, '');
              }

              return path;
            }),
          } as ContextAnnotation);

          dataStream.writeData({
            type: 'progress',
            label: 'context',
            status: 'complete',
            order: progressCounter++,
            message: 'Code Files Selected',
          } satisfies ProgressAnnotation);

          // logger.debug('Code Files Selected');
        }

        let subAgentPlan: string | undefined = undefined;
        let plannerAgentId: string | undefined = undefined;

        if (chatMode === 'build') {
          writeCommentary('plan', 'I am drafting a clear step-by-step plan before coding.');

          const latestPlannerSourceMessage = [...processedMessages]
            .reverse()
            .find((message) => message.role === 'user');

          const plannerSelection = latestPlannerSourceMessage
            ? extractPropertiesFromMessage(latestPlannerSourceMessage)
            : undefined;
          const plannerModel = plannerSelection?.model;
          const plannerProvider = plannerSelection?.provider;
          const plannerEnabled = parseBooleanEnv(envVars?.BOLT_PLANNER_SUBAGENT_ENABLED, true);
          const plannerLongThinkEnabled = parseBooleanEnv(envVars?.BOLT_PLANNER_LONG_THINK_ENABLED, false);
          const plannerModelForPolicy = plannerModel || selectedModel || '';
          const plannerIsLongThink = LONG_THINK_MODEL_RE.test(plannerModelForPolicy);
          const shouldRunPlanner = plannerEnabled && (!plannerIsLongThink || plannerLongThinkEnabled);

          if (!shouldRunPlanner) {
            const skipReason = !plannerEnabled
              ? 'Planner helper is disabled by runtime config.'
              : `Planner helper is skipped for long-think model ${plannerModelForPolicy || 'unknown model'} to keep execution responsive.`;
            writeCommentary(
              'plan',
              'I am moving straight into implementation for this run.',
              'in-progress',
              `Key changes: ${skipReason}
Next: I am executing code actions directly and will stream progress checkpoints.`,
            );
          }

          if (shouldRunPlanner) {
            const getPlannerParams = async (_messages: Messages, _config: SubAgentConfig) => ({
              env: runtimeEnv as any,
              options: {
                maxSteps: 1,
                tools: {},
                toolChoice: undefined,
              } as StreamingOptions,
              apiKeys,
              files,
              providerSettings,
              promptId,
              contextOptimization,
              contextFiles: filteredFiles,
              summary,
              messageSliceId,
              chatMode: 'discuss',
              designScheme,
              projectMemory: effectiveProjectMemory || undefined,
            });

            const plannerExecutor = createPlannerExecutor(getPlannerParams);
            subAgentManager.registerExecutor('planner', plannerExecutor);

            try {
              plannerAgentId = await subAgentManager.spawn(undefined, {
                type: 'planner',
                model: plannerModel,
                provider: plannerProvider,
              });

              const onProgress = (state: SubAgentState, _output: string) => {
                if (state === 'planning') {
                  writeCommentary('plan', 'I am breaking your request into practical steps.');
                } else if (state === 'executing') {
                  writeCommentary('plan', 'I am finalizing the plan and preparing to execute.');
                }
              };

              const plannerResult = await subAgentManager.start(plannerAgentId, processedMessages, onProgress);

              if (plannerResult.metadata.tokenUsage) {
                addUsageTotals(cumulativeUsage, plannerResult.metadata.tokenUsage);
              }

              if (plannerResult.success && plannerResult.output) {
                subAgentPlan = plannerResult.output;

                const subAgentEvent: SubAgentEvent = {
                  type: 'sub-agent',
                  agentId: plannerResult.metadata.id,
                  agentType: plannerResult.metadata.type,
                  state: plannerResult.metadata.state,
                  model: plannerResult.metadata.model,
                  provider: plannerResult.metadata.provider,
                  plan: plannerResult.metadata.plan,
                  createdAt: plannerResult.metadata.createdAt,
                  startedAt: plannerResult.metadata.startedAt,
                  completedAt: plannerResult.metadata.completedAt,
                  tokenUsage: plannerResult.metadata.tokenUsage,
                };

                dataStream.writeData(subAgentEvent);

                writeCommentary(
                  'plan',
                  'Planning is complete. I am moving into execution now.',
                  'complete',
                  subAgentPlan.slice(0, 260),
                );
              }
            } catch {
              writeCommentary(
                'recovery',
                'Planning helper had an issue, so I am continuing directly.',
                'warning',
                `Key changes: The planning helper could not complete this step, so I switched to direct execution.
Next: I am continuing with the main coding flow and will keep you updated.`,
              );
            }
          }
        }

        const options: StreamingOptions = {
          supabaseConnection: supabase,
          toolChoice: 'auto',
          tools: mcpService.toolsWithoutExecute,
          maxSteps: maxLLMSteps,
          onChunk: () => {
            markRunActivity();
          },
          onError: ({ error }) => {
            logger.error('Streaming error:', error);
          },
          onStepFinish: ({ toolCalls, toolResults }) => {
            markRunActivity();

            // add tool call annotations for frontend processing
            toolCalls.forEach((toolCall) => {
              mcpService.processToolCall(toolCall, dataStream);
            });

            const normalizedToolResults = (toolResults as Array<Record<string, unknown>> | undefined) ?? [];

            if (toolCalls.length > 0 || (toolResults?.length ?? 0) > 0) {
              const toolNames = toolCalls.map((call) => call.toolName).join(', ');
              writeCommentary(
                'verification',
                'I finished a step and I am checking the result before continuing.',
                'in-progress',
                toolNames
                  ? `Key changes: Finished actions (${toolNames}) and collected ${(toolResults?.length ?? 0).toString()} updates.
Next: I am confirming everything still works before the next step.`
                  : `Key changes: Finished one step and collected ${(toolResults?.length ?? 0).toString()} updates.
Next: I am confirming everything still works before moving on.`,
              );
            }

            const checkpointEvents = extractCheckpointEvents({
              toolCalls: toolCalls as Array<{ toolName?: string; toolCallId?: string; args?: unknown }>,
              toolResults: normalizedToolResults as Array<{
                toolName?: string;
                toolCallId?: string;
                result?: unknown;
              }>,
            });

            checkpointEvents.forEach((event) => dataStream.writeData({ ...(event as CheckpointDataEvent) }));

            const executionFailure = extractExecutionFailure({
              toolCalls: toolCalls as Array<{ toolName?: string; toolCallId?: string; args?: unknown }>,
              toolResults: normalizedToolResults as Array<{
                toolName?: string;
                toolCallId?: string;
                result?: unknown;
              }>,
            });

            if (executionFailure) {
              hasExecutionFailures = true;
              latestExecutionFailure = executionFailure;
              recoveryTriggered = true;
              writeCommentary(
                'recovery',
                'A step failed. I am checking it now and applying a fix automatically.',
                'warning',
                `Key changes: One of the recent actions did not succeed.
Next: I will recover and continue from the latest stable point.`,
              );
            }

            const recoverySignal = recoveryController.analyzeStep(
              toolCalls.map((call) => ({ toolName: call.toolName, args: call.args })),
              normalizedToolResults.length,
            );

            if (recoverySignal) {
              pendingRecoveryReason = pendingRecoveryReason || recoverySignal.reason;
              pendingRecoveryBackoffMs = Math.max(pendingRecoveryBackoffMs, recoverySignal.backoffMs);
              forceFinalizeRequested = forceFinalizeRequested || recoverySignal.forceFinalize;
              recoveryTriggered = true;
              writeCommentary('recovery', recoverySignal.message, 'warning', recoverySignal.detail);
            }

            if (normalizedToolResults.length) {
              for (const toolResult of normalizedToolResults) {
                collectedToolOutputs.push(
                  JSON.stringify({
                    toolName: toolResult.toolName,
                    toolCallId: toolResult.toolCallId,
                    result: toolResult.result,
                  }),
                );
              }
            }
          },
          onFinish: async ({ text: content, finishReason, usage }) => {
            logger.debug('usage', JSON.stringify(usage));

            if (usage) {
              addUsageTotals(cumulativeUsage, usage as any);
            }

            const lastUserMessage = processedMessages.filter((x) => x.role == 'user').slice(-1)[0];
            const extractedLastUser = extractPropertiesFromMessage(lastUserMessage);
            const { model, provider } = extractedLastUser;
            const lastUserContent =
              typeof extractedLastUser.content === 'string'
                ? extractedLastUser.content
                : JSON.stringify(extractedLastUser.content);
            const shouldForceFinalize = finishReason === 'tool-calls' || forceFinalizeRequested;

            if (shouldForceFinalize && !forceFinalizeAttempted) {
              forceFinalizeAttempted = true;

              if (pendingRecoveryBackoffMs > 0) {
                writeCommentary(
                  'recovery',
                  'I am taking a short recovery pause before the next step.',
                  'warning',
                  `Key changes: A quick safety pause is in progress.
Next: I will continue automatically right after this pause.`,
                );
                await new Promise((resolve) => setTimeout(resolve, pendingRecoveryBackoffMs));
              }

              writeCommentary(
                'next-step',
                pendingRecoveryReason
                  ? 'Recovery is complete. I am preparing your final answer now.'
                  : 'Execution is complete. I am preparing your final answer now.',
                pendingRecoveryReason ? 'recovered' : 'in-progress',
              );

              const toolSummary =
                collectedToolOutputs.length > 0
                  ? collectedToolOutputs.slice(-6).join('\n')
                  : '(no tool results captured)';

              processedMessages.push({ id: generateId(), role: 'assistant', content });
              processedMessages.push({
                id: generateId(),
                role: 'user',
                content: `[Model: ${model}]

[Provider: ${provider}]

You already gathered tool outputs. Now provide the final answer without any more tool calls.
If the user asked for a markdown file, create it using <boltAction type="file">.
${pendingRecoveryReason ? `Recovery reason: ${pendingRecoveryReason}. Summarize progress and continue.` : ''}

Tool outputs:
${toolSummary}`,
              });

              const finalizeOptions: StreamingOptions = {
                ...options,
                maxSteps: 1,
                tools: {},
                toolChoice: undefined,
                onStepFinish: undefined,
                onFinish: ({ text: finalContent, usage: finalizeUsage }) => {
                  if (finalizeUsage) {
                    addUsageTotals(cumulativeUsage, finalizeUsage as any);
                  }

                  if (pendingRecoveryReason) {
                    recoverySucceeded = true;
                    writeCommentary(
                      'recovery',
                      'Recovery finished successfully.',
                      'recovered',
                      `Key changes: Recovery completed and the workflow is stable again.
Next: I am sending the final result now.`,
                    );
                    pendingRecoveryReason = undefined;
                    pendingRecoveryBackoffMs = 0;
                    forceFinalizeRequested = false;
                  }

                  if (hasExecutionFailures && latestExecutionFailure) {
                    writeCommentary(
                      'next-step',
                      'Work finished, but one step still needs attention.',
                      'warning',
                      `Key changes: A previous step did not finish successfully.
Next: I am returning clear recovery instructions to help you resolve it quickly.`,
                    );
                  } else {
                    writeCommentary('next-step', 'Final response generated and ready for delivery.', 'complete');
                  }

                  stopRunMonitors();
                  emitRunCompletionEvents(finalContent, model, provider);
                },
              };

              const result = await streamText({
                messages: [...processedMessages],
                env: runtimeEnv as any,
                options: {
                  ...finalizeOptions,
                  abortSignal: createStreamAbortSignal(),
                },
                apiKeys,
                files,
                providerSettings,
                promptId,
                contextOptimization,
                contextFiles: filteredFiles,
                summary,
                messageSliceId,
                chatMode,
                designScheme,
                projectMemory: effectiveProjectMemory || undefined,
                subAgentPlan,
              });

              markRunActivity();
              result.mergeIntoDataStream(dataStream);

              return;
            }

            const runContinuationDecision = analyzeRunContinuation({
              chatMode: chatMode || 'build',
              lastUserContent,
              assistantContent: content,
              alreadyAttempted: false,
              attemptCount: runContinuationAttempts,
            });
            const continuationLimitReached = runContinuationAttempts >= MAX_RUN_CONTINUATION_ATTEMPTS;
            const shouldContinueForRunIntent = runContinuationDecision.shouldContinue && !continuationLimitReached;

            if (shouldContinueForRunIntent) {
              runContinuationAttempts += 1;

              const continuationAttemptLabel = `${runContinuationAttempts}/${MAX_RUN_CONTINUATION_ATTEMPTS}`;
              writeCommentary(
                'recovery',
                'I detected that setup finished but the app did not start yet. I will continue automatically.',
                'warning',
                `Key changes: Continuation triggered (${runContinuationDecision.reason}, attempt ${continuationAttemptLabel}). I detected a starter/bootstrap-only response.
Next: I will continue from the existing project state, implement the requested app, and run it.`,
              );
              logger.info(
                `run continuation triggered ${JSON.stringify({
                  runId,
                  reason: runContinuationDecision.reason,
                  provider,
                  model,
                  attempt: runContinuationAttempts,
                  maxAttempts: MAX_RUN_CONTINUATION_ATTEMPTS,
                  assistantChars: content.length,
                  assistantPreview: content.replace(/\s+/g, ' ').slice(0, 220),
                })}`,
              );

              processedMessages.push({ id: generateId(), role: 'assistant', content });
              processedMessages.push({
                id: generateId(),
                role: 'user',
                content: `[Model: ${model}]

[Provider: ${provider}]

You scaffolded a project but did not complete the requested implementation.
Continue now and do ALL of the following:
1) continue from the current project files (do NOT re-run create-vite/create-react-app if package.json already exists).
2) implement the requested product requirements from the original user request:
   ${lastUserContent}
3) install dependencies only if missing.
4) include a <boltAction type="start"> command that launches the dev server.
5) if a command fails, self-heal and retry with a corrected command.
6) your response must start with executable <boltAction> steps (no plan-only prose).
7) if preview still shows the starter, replace src/App.tsx (or equivalent entry UI file) with the requested implementation.
8) keep the final response concise and execution-focused.
9) do NOT run inspection-only commands (ls, pwd, cat, find, tree, env) as standalone steps.
10) your first meaningful action must implement feature code files for the original request.
${runContinuationAttempts >= 2 ? '11) this is the final continuation attempt: skip diagnostics and apply implementation changes immediately.' : ''}
`,
              });

              const result = await streamText({
                messages: [...processedMessages],
                env: runtimeEnv as any,
                options: {
                  ...options,
                  abortSignal: createStreamAbortSignal(),
                },
                apiKeys,
                files,
                providerSettings,
                promptId,
                contextOptimization,
                contextFiles: filteredFiles,
                chatMode,
                designScheme,
                summary,
                messageSliceId,
                projectMemory: effectiveProjectMemory || undefined,
                subAgentPlan,
              });

              markRunActivity();
              result.mergeIntoDataStream(dataStream);

              return;
            }

            if (
              runContinuationDecision.shouldContinue &&
              continuationLimitReached &&
              !forcedRunRecoveryAttempted &&
              (chatMode || 'build') === 'build'
            ) {
              forcedRunRecoveryAttempted = true;
              writeCommentary(
                'recovery',
                'Automatic recovery is escalating because previous attempts did not produce implementation steps.',
                'warning',
                `Key changes: Escalated continuation recovery after ${runContinuationAttempts} attempts (${runContinuationDecision.reason}).
Next: I will force a file-first implementation response and relaunch preview from the existing project state.`,
              );
              logger.warn(
                `run continuation escalated to forced recovery ${JSON.stringify({
                  runId,
                  reason: runContinuationDecision.reason,
                  provider,
                  model,
                  attempts: runContinuationAttempts,
                  maxAttempts: MAX_RUN_CONTINUATION_ATTEMPTS,
                  assistantChars: content.length,
                  assistantPreview: content.replace(/\s+/g, ' ').slice(0, 220),
                })}`,
              );

              processedMessages.push({ id: generateId(), role: 'assistant', content });
              processedMessages.push({
                id: generateId(),
                role: 'user',
                content: buildForcedRunRecoveryPrompt({
                  provider,
                  model,
                  originalRequest: lastUserContent,
                }),
              });

              const result = await streamText({
                messages: [...processedMessages],
                env: runtimeEnv as any,
                options: {
                  ...options,
                  abortSignal: createStreamAbortSignal(),
                },
                apiKeys,
                files,
                providerSettings,
                promptId,
                contextOptimization,
                contextFiles: filteredFiles,
                chatMode,
                designScheme,
                summary,
                messageSliceId,
                projectMemory: effectiveProjectMemory || undefined,
                subAgentPlan,
              });

              markRunActivity();
              result.mergeIntoDataStream(dataStream);

              return;
            }

            logger.debug(
              `run continuation not required ${JSON.stringify({
                runId,
                reason: runContinuationDecision.reason,
                provider,
                model,
                continuationAttempts: runContinuationAttempts,
              })}`,
            );

            if (finishReason !== 'length') {
              if (pendingRecoveryReason) {
                recoverySucceeded = true;
                writeCommentary(
                  'recovery',
                  'Recovery finished successfully.',
                  'recovered',
                  `Key changes: Recovery completed and the workflow is stable again.
Next: I am sending the final result now.`,
                );
                pendingRecoveryReason = undefined;
                pendingRecoveryBackoffMs = 0;
                forceFinalizeRequested = false;
              }

              if (hasExecutionFailures && latestExecutionFailure) {
                writeCommentary(
                  'next-step',
                  'Work finished, but one step still needs attention.',
                  'warning',
                  `Key changes: A previous step did not finish successfully.
Next: I am returning clear recovery instructions to help you resolve it quickly.`,
                );
              } else {
                writeCommentary('next-step', 'Final response generated and ready for delivery.', 'complete');
              }

              stopRunMonitors();
              emitRunCompletionEvents(content, model, provider);
              await new Promise((resolve) => setTimeout(resolve, 0));

              return;
            }

            if (stream.switches >= MAX_RESPONSE_SEGMENTS) {
              throw Error('Cannot continue message: Maximum segments reached');
            }

            const switchesLeft = MAX_RESPONSE_SEGMENTS - stream.switches;

            logger.info(`Reached max token limit (${MAX_TOKENS}): Continuing message (${switchesLeft} switches left)`);

            processedMessages.push({ id: generateId(), role: 'assistant', content });
            processedMessages.push({
              id: generateId(),
              role: 'user',
              content: `[Model: ${model}]\n\n[Provider: ${provider}]\n\n${CONTINUE_PROMPT}`,
            });

            const result = await streamText({
              messages: [...processedMessages],
              env: runtimeEnv as any,
              options: {
                ...options,
                abortSignal: createStreamAbortSignal(),
              },
              apiKeys,
              files,
              providerSettings,
              promptId,
              contextOptimization,
              contextFiles: filteredFiles,
              chatMode,
              designScheme,
              summary,
              messageSliceId,
              projectMemory: effectiveProjectMemory || undefined,
              subAgentPlan,
            });

            markRunActivity();
            result.mergeIntoDataStream(dataStream);

            return;
          },
        };

        dataStream.writeData({
          type: 'progress',
          label: 'response',
          status: 'in-progress',
          order: progressCounter++,
          message: 'Generating Response',
        } satisfies ProgressAnnotation);
        writeCommentary('action', 'I am now executing the plan and streaming progress as I go.');

        const result = await streamText({
          messages: [...processedMessages],
          env: runtimeEnv as any,
          options: {
            ...options,
            abortSignal: createStreamAbortSignal(),
          },
          apiKeys,
          files,
          providerSettings,
          promptId,
          contextOptimization,
          contextFiles: filteredFiles,
          chatMode,
          designScheme,
          summary,
          messageSliceId,
          projectMemory: effectiveProjectMemory || undefined,
          subAgentPlan,
        });

        markRunActivity();
        result.mergeIntoDataStream(dataStream);
      },
      onError: (error: any) => {
        stopHeartbeatIfRunning();

        const elapsedMs = Date.now() - requestStartedAt;
        logger.error(
          `chat stream onError ${JSON.stringify({
            ...requestDebugContext,
            elapsedMs,
            resolvedProvider: resolvedSelectionForLogs.provider,
            resolvedModel: resolvedSelectionForLogs.model,
            errorName: error?.name,
            errorMessage: error?.message || String(error),
          })}`,
        );

        // Provide more specific error messages for common issues
        const errorMessage = error.message || 'Unknown error';

        if (errorMessage.includes('model') && errorMessage.includes('not found')) {
          return 'Custom error: Invalid model selected. Please check that the model name is correct and available.';
        }

        if (errorMessage.includes('BOLT_STREAM_TIMEOUT')) {
          return 'Custom error: Generation stream timed out while waiting for model output. The run was stopped so recovery can continue safely.';
        }

        if (errorMessage.includes('Invalid JSON response')) {
          return 'Custom error: The AI service returned an invalid response. This may be due to an invalid model name, API rate limiting, or server issues. Try selecting a different model or check your API key.';
        }

        if (
          errorMessage.includes('Invalid AWS Bedrock configuration format') ||
          errorMessage.includes('Missing required AWS credentials')
        ) {
          return 'Custom error: Bedrock credentials are invalid. Switch to a configured provider or update Bedrock JSON credentials in Settings.';
        }

        if (errorMessage.includes('Missing API key for')) {
          return 'Custom error: The selected provider is not configured for this instance yet. Select a provider with a valid key and retry.';
        }

        if (
          errorMessage.includes('API key') ||
          errorMessage.includes('unauthorized') ||
          errorMessage.includes('authentication')
        ) {
          return 'Custom error: Invalid or missing API key. Please check your API key configuration.';
        }

        if (errorMessage.includes('token') && errorMessage.includes('limit')) {
          return 'Custom error: Token limit exceeded. The conversation is too long for the selected model. Try using a model with larger context window or start a new conversation.';
        }

        if (errorMessage.includes('rate limit') || errorMessage.includes('429')) {
          return 'Custom error: API rate limit exceeded. Please wait a moment before trying again.';
        }

        if (errorMessage.includes('network') || errorMessage.includes('timeout')) {
          return 'Custom error: Network error. Please check your internet connection and try again.';
        }

        return `Custom error: ${errorMessage}`;
      },
    }).pipeThrough(
      new TransformStream({
        transform: (chunk, controller) => {
          if (!lastChunk) {
            lastChunk = ' ';
          }

          if (typeof chunk === 'string') {
            if (chunk.startsWith('g') && !lastChunk.startsWith('g')) {
              controller.enqueue(encoder.encode(`0: "<div class=\\"__boltThought__\\">"\n`));
            }

            if (lastChunk.startsWith('g') && !chunk.startsWith('g')) {
              controller.enqueue(encoder.encode(`0: "</div>\\n"\n`));
            }
          }

          lastChunk = chunk;

          let transformedChunk = chunk;

          if (typeof chunk === 'string' && chunk.startsWith('g')) {
            let content = chunk.split(':').slice(1).join(':');

            if (content.endsWith('\n')) {
              content = content.slice(0, content.length - 1);
            }

            transformedChunk = `0:${content}\n`;
          }

          // Convert the string stream to a byte stream
          const str = typeof transformedChunk === 'string' ? transformedChunk : JSON.stringify(transformedChunk);
          controller.enqueue(encoder.encode(str));
        },
      }),
    );

    return new Response(dataStream, {
      status: 200,
      headers: {
        'Content-Type': 'text/event-stream; charset=utf-8',
        Connection: 'keep-alive',
        'Cache-Control': 'no-cache',
        'Text-Encoding': 'chunked',
      },
    });
  } catch (error: any) {
    stopHeartbeatIfRunning();

    const elapsedMs = Date.now() - requestStartedAt;
    logger.error('chat request failed before stream completion', {
      ...requestDebugContext,
      elapsedMs,
      resolvedProvider: resolvedSelectionForLogs.provider,
      resolvedModel: resolvedSelectionForLogs.model,
      errorName: error?.name,
      errorMessage: error?.message || String(error),
      errorStack: error?.stack,
    });

    const errorResponse = {
      error: true,
      message: error.message || 'An unexpected error occurred',
      statusCode: error.statusCode || 500,
      isRetryable: error.isRetryable !== false, // Default to retryable unless explicitly false
      provider: error.provider || 'unknown',
    };

    if (error.message?.includes('API key')) {
      return new Response(
        JSON.stringify({
          ...errorResponse,
          message: 'Invalid or missing API key',
          statusCode: 401,
          isRetryable: false,
        }),
        {
          status: 401,
          headers: { 'Content-Type': 'application/json' },
          statusText: 'Unauthorized',
        },
      );
    }

    return new Response(JSON.stringify(errorResponse), {
      status: errorResponse.statusCode,
      headers: { 'Content-Type': 'application/json' },
      statusText: 'Error',
    });
  }
}
